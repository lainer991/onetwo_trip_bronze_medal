{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import Colours\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_transf.csv')\n",
    "test = pd.read_csv('data/test_transf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе решения сравнивались несколько алгоритмов, также тестировался стэкинг нескольких алгоритмов. Лучше всего себя показал LGB с байесовской оптимизацией гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(filter(lambda x: ('field' in x), train.columns))\n",
    "\n",
    "\n",
    "X = train[features]\n",
    "y = train['goal1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_cv(n_estimators, learning_rate, max_depth,\\\n",
    "           num_leaves,\\\n",
    "           colsample_bytree,\\\n",
    "           subsample,\\\n",
    "           reg_alpha,\\\n",
    "           reg_lambda,\\\n",
    "           \n",
    "           data, targets):\n",
    "\n",
    "    estimator = LGBMClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        num_leaves= num_leaves,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        \n",
    "        random_state=2\n",
    "    )\n",
    "    cval = cross_val_score(estimator, data, targets,\n",
    "                               scoring='neg_log_loss', cv=4)\n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lgb(data, targets):\n",
    "    \"\"\"Apply Bayesian Optimization to Lgb parameters.\"\"\"\n",
    "    def lgb_crossval(n_estimators, learning_rate, max_depth, \n",
    "                    num_leaves,\n",
    "                    colsample_bytree,\n",
    "                    subsample,\n",
    "                    reg_alpha,\n",
    "                    reg_lambda):\n",
    "        \"\"\"Wrapper of Lgb validation.\n",
    "        Notice how we ensure n_estimators and min_samples_split are casted\n",
    "        to integer before we pass them along. Moreover, to avoid max_features\n",
    "        taking values outside the (0, 1) range, we also ensure it is capped\n",
    "        accordingly.\n",
    "        \"\"\"\n",
    "        return lgb_cv(\n",
    "            n_estimators=int(n_estimators),\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=int(max_depth),\n",
    "            \n",
    "            num_leaves= int(num_leaves),\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            subsample=subsample,\n",
    "            reg_alpha=int(reg_alpha),\n",
    "            reg_lambda=int(reg_lambda) ,            \n",
    "            \n",
    "            data=data,\n",
    "            targets=targets,\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lgb_crossval,\n",
    "        pbounds={\n",
    "            \"n_estimators\": (200, 250),\n",
    "            \"learning_rate\": (0.01, 0.05),\n",
    "            \"max_depth\":(1, 8),\n",
    "            \n",
    "            'num_leaves':(6,50),\n",
    "            'colsample_bytree':(0.5,0.7),\n",
    "            'subsample':(0.6,0.8),\n",
    "            'reg_alpha':(0,100),\n",
    "            'reg_lambda':(0,100),\n",
    "        },\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(n_iter=5)\n",
    "\n",
    "    print(\"Final result:\", optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m--- Optimizing Algo ---\u001b[0m\n",
      "|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1014  \u001b[0m | \u001b[0m 0.5383  \u001b[0m | \u001b[0m 0.03488 \u001b[0m | \u001b[0m 4.064   \u001b[0m | \u001b[0m 239.3   \u001b[0m | \u001b[0m 40.32   \u001b[0m | \u001b[0m 27.26   \u001b[0m | \u001b[0m 27.65   \u001b[0m | \u001b[0m 0.7604  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.102   \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.04504 \u001b[0m | \u001b[0m 3.505   \u001b[0m | \u001b[0m 225.0   \u001b[0m | \u001b[0m 36.07   \u001b[0m | \u001b[0m 71.27   \u001b[0m | \u001b[0m 37.03   \u001b[0m | \u001b[0m 0.7122  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.1025  \u001b[0m | \u001b[0m 0.6006  \u001b[0m | \u001b[0m 0.01055 \u001b[0m | \u001b[0m 6.41    \u001b[0m | \u001b[0m 244.1   \u001b[0m | \u001b[0m 22.05   \u001b[0m | \u001b[0m 61.54   \u001b[0m | \u001b[0m 7.538   \u001b[0m | \u001b[0m 0.6738  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1019  \u001b[0m | \u001b[0m 0.6866  \u001b[0m | \u001b[0m 0.03606 \u001b[0m | \u001b[0m 3.78    \u001b[0m | \u001b[0m 239.4   \u001b[0m | \u001b[0m 19.94   \u001b[0m | \u001b[0m 56.81   \u001b[0m | \u001b[0m 86.91   \u001b[0m | \u001b[0m 0.6872  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1025  \u001b[0m | \u001b[0m 0.6604  \u001b[0m | \u001b[0m 0.01575 \u001b[0m | \u001b[0m 5.93    \u001b[0m | \u001b[0m 235.2   \u001b[0m | \u001b[0m 15.63   \u001b[0m | \u001b[0m 92.49   \u001b[0m | \u001b[0m 44.21   \u001b[0m | \u001b[0m 0.7819  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.102   \u001b[0m | \u001b[0m 0.6468  \u001b[0m | \u001b[0m 0.01578 \u001b[0m | \u001b[0m 5.419   \u001b[0m | \u001b[0m 201.4   \u001b[0m | \u001b[0m 7.683   \u001b[0m | \u001b[0m 0.7962  \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 0.6532  \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.1012  \u001b[0m | \u001b[95m 0.663   \u001b[0m | \u001b[95m 0.04751 \u001b[0m | \u001b[95m 3.716   \u001b[0m | \u001b[95m 201.3   \u001b[0m | \u001b[95m 48.75   \u001b[0m | \u001b[95m 0.0453  \u001b[0m | \u001b[95m 0.9581  \u001b[0m | \u001b[95m 0.6061  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.1011  \u001b[0m | \u001b[95m 0.6258  \u001b[0m | \u001b[95m 0.03018 \u001b[0m | \u001b[95m 6.067   \u001b[0m | \u001b[95m 247.5   \u001b[0m | \u001b[95m 46.08   \u001b[0m | \u001b[95m 0.4018  \u001b[0m | \u001b[95m 98.02   \u001b[0m | \u001b[95m 0.6677  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1014  \u001b[0m | \u001b[0m 0.6599  \u001b[0m | \u001b[0m 0.01366 \u001b[0m | \u001b[0m 7.635   \u001b[0m | \u001b[0m 249.0   \u001b[0m | \u001b[0m 12.15   \u001b[0m | \u001b[0m 0.09091 \u001b[0m | \u001b[0m 3.343   \u001b[0m | \u001b[0m 0.7286  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.1021  \u001b[0m | \u001b[0m 0.5935  \u001b[0m | \u001b[0m 0.02263 \u001b[0m | \u001b[0m 2.393   \u001b[0m | \u001b[0m 247.3   \u001b[0m | \u001b[0m 48.71   \u001b[0m | \u001b[0m 0.09543 \u001b[0m | \u001b[0m 97.68   \u001b[0m | \u001b[0m 0.6017  \u001b[0m |\n",
      "=========================================================================================================================\n",
      "Final result: {'target': -0.1011068839628719, 'params': {'colsample_bytree': 0.6258303289594727, 'learning_rate': 0.030177890484495297, 'max_depth': 6.066542556296102, 'n_estimators': 247.48325277880315, 'num_leaves': 46.07650862859507, 'reg_alpha': 0.40182576945416715, 'reg_lambda': 98.01862813537362, 'subsample': 0.6677186428477122}}\n"
     ]
    }
   ],
   "source": [
    "print(Colours.green(\"--- Optimizing Algo ---\"))\n",
    "optimize_lgb(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6864370237863696 ± 0.008963339327280133\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=247, learning_rate = 0.03, max_depth = 6,\n",
    "                        colsample_bytree = 0.626,\n",
    "                        subsample = 0.668,\n",
    "                        num_leaves = 46,\n",
    "                        reg_alpha = 0.4,\n",
    "                        reg_lambda = 98\n",
    "                        )\n",
    "scores = cross_val_score(lgb, X, y, scoring='roc_auc', cv=3)\n",
    "print(f\"{scores.mean()} ± {scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit\n",
    "lgb.fit(X, y)\n",
    "pred = lgb.predict_proba(test[features])[:,1]\n",
    "\n",
    "pd.DataFrame(pred, columns=['proba'], index=test['orderid']).to_csv('submit_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данного решения хватило для получения бронзовой медали в первой задаче контеста. Модель можно улучшить, если продолжить более детальный анализ закодированных признаков. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
